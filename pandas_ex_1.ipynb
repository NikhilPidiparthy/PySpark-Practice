{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/26 14:06:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Pandas Ex\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>cname</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>amount</th>\n",
       "      <th>discount</th>\n",
       "      <th>member</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Niki</td>\n",
       "      <td>2021-07-22 01:01:53</td>\n",
       "      <td>2415.12</td>\n",
       "      <td>8 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Raaki</td>\n",
       "      <td>2021-08-13 12:15:33</td>\n",
       "      <td>9399.22</td>\n",
       "      <td>10 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Maaki</td>\n",
       "      <td>2022-08-18 22:12:45</td>\n",
       "      <td>1384.16</td>\n",
       "      <td>20 %</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Nibba</td>\n",
       "      <td>2022-07-08 14:18:37</td>\n",
       "      <td>2246.00</td>\n",
       "      <td>9 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Nibbi</td>\n",
       "      <td>2023-01-04 07:13:13</td>\n",
       "      <td>3145.13</td>\n",
       "      <td>12 %</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cid  cname             dateTime   amount discount  member\n",
       "0  101   Niki  2021-07-22 01:01:53  2415.12      8 %    True\n",
       "1  102  Raaki  2021-08-13 12:15:33  9399.22     10 %    True\n",
       "2  103  Maaki  2022-08-18 22:12:45  1384.16     20 %   False\n",
       "3  104  Nibba  2022-07-08 14:18:37  2246.00      9 %    True\n",
       "4  105  Nibbi  2023-01-04 07:13:13  3145.13     12 %   False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df1 = pd.read_csv(\"DataSets/transactions_1.csv\")\n",
    "p_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cid           int64\n",
       "cname        object\n",
       "dateTime     object\n",
       "amount      float64\n",
       "discount     object\n",
       "member         bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------------+-------+--------+------+\n",
      "|cid|cname|           dateTime| amount|discount|member|\n",
      "+---+-----+-------------------+-------+--------+------+\n",
      "|101| Niki|2021-07-22 01:01:53|2415.12|     8 %|  true|\n",
      "|102|Raaki|2021-08-13 12:15:33|9399.22|    10 %|  true|\n",
      "|103|Maaki|2022-08-18 22:12:45|1384.16|    20 %| false|\n",
      "|104|Nibba|2022-07-08 14:18:37| 2246.0|     9 %|  true|\n",
      "|105|Nibbi|2023-01-04 07:13:13|3145.13|    12 %| false|\n",
      "+---+-----+-------------------+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df1 = spark.createDataFrame(p_df1)\n",
    "s_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>cname</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>amount</th>\n",
       "      <th>discount</th>\n",
       "      <th>member</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>ANiki</td>\n",
       "      <td>2021-07-22 01:01:53</td>\n",
       "      <td>2415.12</td>\n",
       "      <td>8 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>BRaaki</td>\n",
       "      <td>2021-08-13 12:15:33</td>\n",
       "      <td>9399.22</td>\n",
       "      <td>10 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108</td>\n",
       "      <td>CMaaki</td>\n",
       "      <td>2022-08-18 22:12:45</td>\n",
       "      <td>1384.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109</td>\n",
       "      <td>DNibba</td>\n",
       "      <td>2022-07-08 14:18:37</td>\n",
       "      <td>2246.00</td>\n",
       "      <td>9 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>ENibbi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3145.13</td>\n",
       "      <td>12 %</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cid   cname             dateTime   amount discount  member\n",
       "0  106   ANiki  2021-07-22 01:01:53  2415.12      8 %    True\n",
       "1  107  BRaaki  2021-08-13 12:15:33  9399.22     10 %    True\n",
       "2  108  CMaaki  2022-08-18 22:12:45  1384.16      NaN   False\n",
       "3  109  DNibba  2022-07-08 14:18:37  2246.00      9 %    True\n",
       "4  110  ENibbi                  NaN  3145.13     12 %   False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df2 = pd.read_csv(\"DataSets/transactions_2.csv\")\n",
    "p_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cid           int64\n",
       "cname        object\n",
       "dateTime     object\n",
       "amount      float64\n",
       "discount     object\n",
       "member         bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field discount: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m s_df2 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_df2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:673\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    670\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[1;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:340\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    339\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 340\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m    701\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    702\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), schema\u001b[39m.\u001b[39mjson())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:512\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    509\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 512\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m    513\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    514\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:439\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan not infer schema from empty dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 439\u001b[0m schema \u001b[39m=\u001b[39m reduce(_merge_type, (_infer_schema(row, names) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data))\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/types.py:1109\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1108\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m b\u001b[39m.\u001b[39mfields)\n\u001b[0;32m-> 1109\u001b[0m     fields \u001b[39m=\u001b[39m [StructField(f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39mdataType, nfs\u001b[39m.\u001b[39mget(f\u001b[39m.\u001b[39mname, NullType()),\n\u001b[1;32m   1110\u001b[0m                                               name\u001b[39m=\u001b[39mnew_name(f\u001b[39m.\u001b[39mname)))\n\u001b[1;32m   1111\u001b[0m               \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields]\n\u001b[1;32m   1112\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[1;32m   1113\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/types.py:1109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1108\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m b\u001b[39m.\u001b[39mfields)\n\u001b[0;32m-> 1109\u001b[0m     fields \u001b[39m=\u001b[39m [StructField(f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39;49mdataType, nfs\u001b[39m.\u001b[39;49mget(f\u001b[39m.\u001b[39;49mname, NullType()),\n\u001b[1;32m   1110\u001b[0m                                               name\u001b[39m=\u001b[39;49mnew_name(f\u001b[39m.\u001b[39;49mname)))\n\u001b[1;32m   1111\u001b[0m               \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields]\n\u001b[1;32m   1112\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[1;32m   1113\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/types.py:1104\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m a\n\u001b[1;32m   1102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(a) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mtype\u001b[39m(b):\n\u001b[1;32m   1103\u001b[0m     \u001b[39m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mCan not merge type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(a), \u001b[39mtype\u001b[39m(b))))\n\u001b[1;32m   1106\u001b[0m \u001b[39m# same type\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n",
      "\u001b[0;31mTypeError\u001b[0m: field discount: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "s_df2 = spark.createDataFrame(p_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------+-------+--------+------+\n",
      "|cid| cname|           dateTime| amount|discount|member|\n",
      "+---+------+-------------------+-------+--------+------+\n",
      "|106| ANiki|2021-07-22 01:01:53|2415.12|     8 %|  True|\n",
      "|107|BRaaki|2021-08-13 12:15:33|9399.22|    10 %|  True|\n",
      "|108|CMaaki|2022-08-18 22:12:45|1384.16|     nan| False|\n",
      "|109|DNibba|2022-07-08 14:18:37| 2246.0|     9 %|  True|\n",
      "|110|ENibbi|                nan|3145.13|    12 %| False|\n",
      "+---+------+-------------------+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df2 = spark.createDataFrame(p_df2.astype('str'))\n",
    "s_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we can convert pandas df to spark df with our own Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------+-------+--------+------+\n",
      "|Cid| CName|           DateTime| Amount|Discount|Member|\n",
      "+---+------+-------------------+-------+--------+------+\n",
      "|106| ANiki|2021-07-22 01:01:53|2415.12|     8 %|  true|\n",
      "|107|BRaaki|2021-08-13 12:15:33|9399.22|    10 %|  true|\n",
      "|108|CMaaki|2022-08-18 22:12:45|1384.16|     NaN| false|\n",
      "|109|DNibba|2022-07-08 14:18:37| 2246.0|     9 %|  true|\n",
      "|110|ENibbi|                NaN|3145.13|    12 %| false|\n",
      "+---+------+-------------------+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField,StructType,BooleanType,FloatType,IntegerType,StringType\n",
    "\n",
    "own_schema = StructType([\n",
    "    StructField(\"Cid\",IntegerType(),True),\n",
    "    StructField(\"CName\",StringType(),True),\n",
    "    StructField(\"DateTime\",StringType(),True),\n",
    "    StructField(\"Amount\",FloatType(),True),\n",
    "    StructField(\"Discount\",StringType(),True),\n",
    "    StructField(\"Member\",BooleanType(),True)\n",
    "])\n",
    "\n",
    "s_df2 = spark.createDataFrame(p_df2,schema=own_schema)\n",
    "s_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Cid: integer (nullable = true)\n",
      " |-- CName: string (nullable = true)\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- Amount: float (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Member: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
