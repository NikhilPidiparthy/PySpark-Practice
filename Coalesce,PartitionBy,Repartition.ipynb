{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/28 21:38:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Coal,Part,Repart\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "|Customer_No|    Card_type|    Date|           Category|Transaction Type| Amount|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "|    1000501|Platinum Card|1/1/2018|           Shopping|           debit|  11.11|\n",
      "|    1000501|     Checking|1/2/2018|    Mortgage & Rent|           debit|1247.44|\n",
      "|    1000501|  Silver Card|1/2/2018|        Restaurants|           debit|  24.22|\n",
      "|    1000501|Platinum Card|1/3/2018|Credit Card Payment|          credit|2298.09|\n",
      "|    1000501|Platinum Card|1/4/2018|      Movies & DVDs|           debit|  11.76|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"DataSets/transactions.csv\",header=True)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Customer_No|\n",
      "+-----------+\n",
      "|    1001863|\n",
      "|    1001368|\n",
      "|    1000210|\n",
      "|    1000531|\n",
      "|    1000654|\n",
      "|    1002324|\n",
      "|    1000501|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"Customer_No\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repartition\n",
    "\n",
    "with this we can increase, decrease the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.repartition(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.repartition(\"Customer_No\").rdd.getNumPartitions()\n",
    "# It can done repartition based on spark optimal number of partitions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.repartition(3,\"Customer_No\").rdd.getNumPartitions()\n",
    "\n",
    "# here I am giving 3 as number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Partition ID|count|\n",
      "+------------+-----+\n",
      "|           0|   31|\n",
      "|           1|   14|\n",
      "|           2|   60|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.repartition(3,\"Customer_No\").withColumn(\"Partition ID\",spark_partition_id()).groupBy(\"Partition ID\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coalesce\n",
    "\n",
    "with this we can only decrease the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.repartition(4).coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.repartition(4).coalesce(6).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## partitionBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7f970c120b80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.write.partitionBy(\"Customer_No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.partitionBy(\"Customer_No\").format(\"csv\").option(\"header\",\"true\").save(\"Hello_Transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important points:\n",
    "\n",
    "repartition:\n",
    "It store the data in memory, it involves in shuffle\n",
    "\n",
    "\n",
    "\n",
    "partitionBy:\n",
    "It store the data directly in disc, no shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the cases, coalesce() does not trigger a shuffle. The coalesce() can be used soon after heavy filtering to optimize the execution time. It is important to notice that coalesce() does not always avoid shuffling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy is for writing the df into separate subdirectories based on partition column, and writes directly to the disc.\n",
    "\n",
    "While repartition and colasce deal with distribution of data inmemory among executors. \n",
    "\n",
    "coalesce is used to decrease the partitions and it may avoid full reshuffling, so it will be faster than repartition.\n",
    "\n",
    "repartition can both increase or decrease partitions but it does with full reshuffle. \n",
    "\n",
    "But if you decrease the partition too much than capacity of executor, it can lead to OOM(Out Of Memory issue). \n",
    "\n",
    "So it depends on the problem which we are solving ie if we want to reduce skewness or distribute more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
